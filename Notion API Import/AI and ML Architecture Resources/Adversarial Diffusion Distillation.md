---
title: Adversarial Diffusion Distillation
database: AI and ML Architecture Resources
created: "2023-12-04T10:30:00.000Z"
updated: "2023-12-04T10:30:00.000Z"
notion_id: 9aa1483e-1abc-4e6b-a17f-96ab1ffe7614
notion_url: "https://www.notion.so/Adversarial-Diffusion-Distillation-9aa1483e1abc4e6ba17f96ab1ffe7614"
code_availability: 
impact_factor: Not Specified
dataset_used: Not Specified
followup_actions: Not Specified
technology_domain: Image Generation
publication_date: 2023-12-04
methodology: Ablation Study, Score Distillation Loss
abstractsummary: Adversarial Diffusion Distillation (ADD) is a novel training approach for efficiently sampling large-scale foundational image diffusion models in 1-4 steps, maintaining high image quality. It leverages score distillation and adversarial loss to achieve high fidelity in low-step sampling regimes, outperforming existing few-step methods and reaching the performance of state-of-the-art diffusion models.
notes: Introduces a method combining adversarial and score distillation objectives to distill pretrained diffusion models for fast, few-step image generation.
attachments: 
application_to_projects: Real-time image generation with foundation models
tags: Diffusion Models, adversarial diffusion distillation, image quality, score distillation, adversarial loss
relevance_score: 8
sourcelink: 
resultsfindings: ADD outperforms existing few-step methods and multi-step generators like SDXL and OpenMUSE, especially in ultra-fast sampling regimes (1-2 steps).
authors: Axel Sauer, Dominik Lorenz, Andreas Blattmann, Robin Rombach
cited_by: Not Specified
contributors: Stability AI
---

