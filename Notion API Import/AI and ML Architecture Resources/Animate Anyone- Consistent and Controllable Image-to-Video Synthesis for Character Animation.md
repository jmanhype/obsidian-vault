---
title: "Animate Anyone: Consistent and Controllable Image-to-Video Synthesis for Character Animation"
database: AI and ML Architecture Resources
created: "2023-12-05T04:09:00.000Z"
updated: "2023-12-05T04:09:00.000Z"
notion_id: d6b8893e-776a-4b30-90be-31ff97817aa8
notion_url: "https://www.notion.so/Animate-Anyone-Consistent-and-Controllable-Image-to-Video-Synthesis-for-Character-Animation-d6b8893e776a4b3090be31ff97817aa8"
code_availability: 
impact_factor: 
dataset_used: 
followup_actions: 
technology_domain: Computer Graphics
publication_date: 
methodology: Research
abstractsummary: This paper presents a novel framework for character animation using diffusion models. It focuses on generating character videos from still images through driving signals, addressing challenges in maintaining temporal consistency and detailed information from characters.
notes: The paper leverages diffusion models for character animation. It introduces ReferenceNet to merge detail features and ensure the preservation of intricate appearance features from reference images. It aims to maintain temporal stability and consistency in character animation.
attachments: /mnt/data/2311.17117.pdf
application_to_projects: 
tags: Character Animation, Image-to-Video Synthesis, Consistency, Controllability, Diffusion Models
relevance_score: 
sourcelink: 
resultsfindings: Development of a novel framework for consistent and controllable character animation.
authors: Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, Liefeng Bo
cited_by: 
contributors: User
---

